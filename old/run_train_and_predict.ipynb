{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"id":"ZQZlMSP_pBaf"},"id":"ZQZlMSP_pBaf","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"d0fb52b1","metadata":{"id":"d0fb52b1"},"outputs":[],"source":["import os\n","import argparse\n","import json\n","import numpy as np\n","import pandas as pd\n","\n","from src.config import cfg\n","from src.fold_train import run_kfold_training\n","from src.ensemble import stacked_predict"]},{"cell_type":"code","execution_count":null,"id":"7983f207","metadata":{"id":"7983f207","outputId":"3c2f6c37-5140-4412-d714-ca57e64a45ab"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of XLMRobertaModel were not initialized from the model checkpoint at MoritzLaurer/multilingual-MiniLMv2-L6-mnli-xnli and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of XLMRobertaModel were not initialized from the model checkpoint at MoritzLaurer/multilingual-MiniLMv2-L6-mnli-xnli and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Train:   0%|          | 0/197 [00:00<?, ?it/s]\n","\n"]},{"ename":"TypeError","evalue":"cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple","output_type":"error","traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m out_dir = args.models_out_dir\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrain_then_predict\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     out_dir = \u001b[43mrun_kfold_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# trains all cfg.models, calibrates, and fits stacker\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mArtifacts at:\u001b[39m\u001b[33m\"\u001b[39m, out_dir)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mpredict\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrain_then_predict\u001b[39m\u001b[33m\"\u001b[39m]:\n","\u001b[36mFile \u001b[39m\u001b[32m~/dev/llm/ensemble_learning/src/fold_train.py:229\u001b[39m, in \u001b[36mrun_kfold_training\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    227\u001b[39m     fold_scores[model_cfg[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = []\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m fold, tr_df, va_df \u001b[38;5;129;01min\u001b[39;00m kfold_splits(df, cfg.n_splits, cfg.seed):\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m         best_f1 = \u001b[43mtrain_model_on_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mva_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemps_weights_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m         fold_scores[model_cfg[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]].append(best_f1)\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.path.join(root_out, \u001b[33m\"\u001b[39m\u001b[33mfold_summary.json\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n","\u001b[36mFile \u001b[39m\u001b[32m~/dev/llm/ensemble_learning/src/fold_train.py:96\u001b[39m, in \u001b[36mtrain_model_on_fold\u001b[39m\u001b[34m(model_cfg, fold_idx, train_df, val_df, out_dir, temps_weights_path)\u001b[39m\n\u001b[32m     94\u001b[39m best_f1, patience_counter, history, min_delta = \u001b[32m0.0\u001b[39m, \u001b[32m0\u001b[39m, [], \u001b[32m0.005\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg.epochs):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     tr_loss, tr_f1 = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m     val_loss, val_f1 = eval_one_epoch(model, val_loader, criterion, device)\n\u001b[32m     98\u001b[39m     history.append({\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m: epoch+\u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m: tr_loss, \u001b[33m\"\u001b[39m\u001b[33mtrain_f1\u001b[39m\u001b[33m\"\u001b[39m: tr_f1, \u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m: val_loss, \u001b[33m\"\u001b[39m\u001b[33mval_f1\u001b[39m\u001b[33m\"\u001b[39m: val_f1})\n","\u001b[36mFile \u001b[39m\u001b[32m~/dev/llm/ensemble_learning/src/engine.py:44\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, scheduler, criterion, scaler, device, grad_clip)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m amp_ctx:\n\u001b[32m     43\u001b[39m     logits = model(input_ids, attention_mask)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m scaler.scale(loss).backward()\n\u001b[32m     47\u001b[39m scaler.unscale_(optimizer)\n","\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n","\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.13/site-packages/torch/nn/modules/loss.py:1310\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1309\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.13/site-packages/torch/nn/functional.py:3462\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3461\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3462\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[31mTypeError\u001b[39m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple"]}],"source":["import sys\n","\n","if \"ipykernel\" in sys.modules:\n","    sys.argv = [\"run_train_and_predict.ipynb\",\n","                \"--mode\", \"predict\",\n","                \"--model_out_dir\", \"outputs/20250922_104022\"]  # to avoid argparse error\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\"--mode\", choices=[\"train\", \"predict\", \"train_then_predict\"], default=\"train_then_predict\")\n","parser.add_argument(\"--test_csv\", default=None, help=\"Override cfg.test_csv for prediction\")\n","parser.add_argument(\"--models_out_dir\", default=None, help=\"If provided, use this trained models root for prediction\")\n","parser.add_argument(\"--save_preds\", default=\"predictions.csv\", help=\"Where to save predictions in predict modes\")\n","args = parser.parse_args()\n","\n","out_dir = args.models_out_dir\n","\n","if args.mode in [\"train\", \"train_then_predict\"]:\n","    out_dir = run_kfold_training()\n","    print(\"Artifacts at:\", out_dir)\n","\n","if args.mode in [\"predict\", \"train_then_predict\"]:\n","    test_csv = args.test_csv or cfg.test_csv\n","    if test_csv is None or not os.path.exists(test_csv):\n","        raise ValueError(f\"Test CSV not found: {test_csv}\")\n","    if out_dir is None or not os.path.exists(out_dir):\n","        raise ValueError(\"Missing models_out_dir to run prediction\")\n","\n","    preds, label_names = stacked_predict(test_csv, out_dir)\n","\n","    df_test = pd.read_csv(test_csv)\n","    if \"id\" in df_test.columns:\n","        sub = pd.DataFrame({\"id\": df_test[\"id\"].values, \"label\": [label_names[i] for i in preds]})\n","    else:\n","        sub = pd.DataFrame({\"label\": [label_names[i] for i in preds]})\n","\n","    sub_path = os.path.join(out_dir, args.save_preds)\n","    sub.to_csv(sub_path, index=False)\n","    print(f\"Saved predictions to {sub_path}\")\n","\n","    np.save(os.path.join(out_dir, \"predictions.npy\"), preds)\n","    with open(os.path.join(out_dir, \"label_names.json\"), \"w\") as f:\n","        json.dump({\"label_names\": label_names}, f, indent=2)\n","print(f\"Saved preds npy and label names under {out_dir}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}